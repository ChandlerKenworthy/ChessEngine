{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import chess.engine\n",
    "import chess.pgn\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../build\")\n",
    "import chess_engine as ce\n",
    "from chess_engine import Board, Generator, print_board, Color, State, Piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(ChessEnv, self).__init__()\n",
    "        NUM_ROWS = 8\n",
    "        NUM_COLS = 8\n",
    "        NUM_ACTIONS = 40 # Number of legal moves\n",
    "\n",
    "        self.MIN_EVAL = -10000\n",
    "        self.MAX_EVAL = 10000\n",
    "\n",
    "        # Define action and observation space\n",
    "        self.action_space = spaces.Discrete(NUM_ACTIONS)\n",
    "        self.observation_space = spaces.Box(low=self.MIN_EVAL, high=self.MAX_EVAL, shape=(NUM_ROWS, NUM_COLS, 12))\n",
    "        self.generator = Generator()\n",
    "        self.board = Board()\n",
    "        self.generator.generate_legal_moves(self.board)\n",
    "        self.engine = chess.engine.SimpleEngine.popen_uci(\"/opt/homebrew/bin/stockfish\")\n",
    "        self.legal_moves = self.generator.get_legal_moves()\n",
    "\n",
    "        # StableBaselines throws error if these are not defined\n",
    "        self.spec = None\n",
    "        self.metadata = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "        # Reset the environment to initial state\n",
    "        self.board.reset()\n",
    "        self.generator.generate_legal_moves(self.board)\n",
    "        self.NUM_ACTIONS = self.generator.get_n_legal_moves()\n",
    "        self.legal_moves = self.generator.get_legal_moves()\n",
    "        self.action_space = spaces.Discrete(self.NUM_ACTIONS)\n",
    "        \n",
    "        return self.get_observation(), {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Take action and return next state, reward, done, info\n",
    "        self.board.make_move(action)\n",
    "\n",
    "        self.generator.generate_legal_moves(self.board) # Updates state of the passed board object\n",
    "        self.legal_moves = self.generator.get_legal_moves()\n",
    "        done = (self.board.get_state() != State.PLAY) or self.generator.get_n_legal_moves() == 0 # Completely winning position (+20 pawns, 2000 centipawns)\n",
    "\n",
    "        if not done:\n",
    "            # Make black take a step, so the agent will only ever play white hence it will become good at maximising the score\n",
    "            # Play a random move\n",
    "            self.board.make_move(np.random.choice(self.legal_moves, 1))\n",
    "            self.generator.generate_legal_moves(self.board) # Updates state of the passed board object\n",
    "            self.legal_moves = self.generator.get_legal_moves()\n",
    "            done = (self.board.get_state() != State.PLAY) or self.generator.get_n_legal_moves() == 0\n",
    "\n",
    "        board = chess.Board(self.board.get_fen())\n",
    "        analysis = self.engine.analyse(board, chess.engine.Limit(time=0.15))\n",
    "        reward = analysis[\"score\"].white().score(mate_score=self.MAX_EVAL)\n",
    "\n",
    "        self.NUM_ACTIONS = self.generator.get_n_legal_moves()\n",
    "        if self.NUM_ACTIONS == 0:\n",
    "            self.NUM_ACTIONS = 1 # \"done\" command now issued anyway so this wont matter\n",
    "        self.action_space = spaces.Discrete(self.NUM_ACTIONS)\n",
    "\n",
    "        return self.get_observation(), reward, done, False, {}\n",
    "    \n",
    "    def get_observation(self):\n",
    "        # Get the current observation/state of the environment\n",
    "        return self.board\n",
    "    \n",
    "    def close(self):\n",
    "        self.engine.quit() # Cleanup memory from stockfish\n",
    "        del self.board\n",
    "        del self.generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Basic Test That Env Works\n",
    "First check my environment I just made is working is a reasonable fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ChessEnv()\n",
    "#observation = env.reset()\n",
    "#action = env.action_space.sample()\n",
    "#observation, reward, done, info = env.step(env.legal_moves[action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ChessAgent:\n",
    "    def __init__(self, \n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.95\n",
    "    ):\n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.training_error = []\n",
    "\n",
    "    def get_action(self, obs: tuple[int, int, bool]) -> int:\n",
    "        \"\"\"\n",
    "        Returns the best action with probability (1 - epsilon)\n",
    "        otherwise a random action with probability epsilon to ensure exploration.\n",
    "        \"\"\"\n",
    "        # with probability epsilon return a random action to explore the environment\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "\n",
    "        # with probability (1 - epsilon) act greedily (exploit)\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "        \n",
    "    def update(\n",
    "        self,\n",
    "        obs: tuple[int, int, bool],\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        terminated: bool,\n",
    "        next_obs: tuple[int, int, bool],\n",
    "    ):\n",
    "        \"\"\"Updates the Q-value of an action.\"\"\"\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
    "        temporal_difference = (\n",
    "            reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n",
    "        )\n",
    "\n",
    "        self.q_values[obs][action] = (\n",
    "            self.q_values[obs][action] + self.lr * temporal_difference\n",
    "        )\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "learning_rate = 0.01\n",
    "n_episodes = 100\n",
    "start_epsilon = 1.0\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2)  # reduce the exploration over time\n",
    "final_epsilon = 0.1\n",
    "\n",
    "agent = ChessAgent(\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitboard_to_array(bitboard):\n",
    "    # Initialize an empty 8x8 numpy array\n",
    "    array = np.zeros((8, 8), dtype=int)\n",
    "    # Iterate over each square on the board\n",
    "    for square in range(64):\n",
    "        # Check if the corresponding bit in the bitboard is set\n",
    "        if bitboard & (1 << square):\n",
    "            # Determine the row and column index for the current square\n",
    "            row = 7 - (square // 8)  # Invert row index to match array indexing\n",
    "            col = square % 8\n",
    "            # Set the value in the numpy array to 1\n",
    "            array[row, col] = 1\n",
    "    return array\n",
    "\n",
    "def board_to_obs(board):\n",
    "    obs = np.zeros((8, 8, 12), dtype=int)\n",
    "    pieces = [Piece.PAWN, Piece.KNIGHT, Piece.BISHOP, Piece.ROOK, Piece.QUEEN, Piece.KING]\n",
    "    colors = [Color.WHITE, Color.BLACK]\n",
    "    # Iterate through each colour\n",
    "    for color in colors:\n",
    "        color_index = 0\n",
    "        if color == Color.BLACK:\n",
    "            color_index = 6 \n",
    "        # Iterate over all types of pieces\n",
    "        for i, piece in enumerate(pieces):\n",
    "            bitboard = board.get_board_color_piece(color, piece)\n",
    "            array = bitboard_to_array(bitboard)\n",
    "            bitboard_index = i + color_index\n",
    "            obs[:, :, bitboard_index] = array\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/var/folders/vr/fcpvsfqx4lvgx3dffz044w2m0000gn/T/ipykernel_2572/2174189920.py:47: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  self.board.make_move(np.random.choice(self.legal_moves, 1))\n",
      " 25%|██▌       | 25/100 [10:08<30:24, 24.33s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(obs)\n\u001b[1;32m     11\u001b[0m take_action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mlegal_moves[action]\n\u001b[0;32m---> 12\u001b[0m next_obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtake_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# update the agent\u001b[39;00m\n\u001b[1;32m     15\u001b[0m agent\u001b[38;5;241m.\u001b[39mupdate(obs, action, reward, terminated, next_obs\u001b[38;5;241m.\u001b[39mget_hash())\n",
      "Cell \u001b[0;32mIn[2], line 53\u001b[0m, in \u001b[0;36mChessEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     50\u001b[0m     done \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard\u001b[38;5;241m.\u001b[39mget_state() \u001b[38;5;241m!=\u001b[39m State\u001b[38;5;241m.\u001b[39mPLAY) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39mget_n_legal_moves() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     52\u001b[0m board \u001b[38;5;241m=\u001b[39m chess\u001b[38;5;241m.\u001b[39mBoard(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard\u001b[38;5;241m.\u001b[39mget_fen())\n\u001b[0;32m---> 53\u001b[0m analysis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLimit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.15\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m reward \u001b[38;5;241m=\u001b[39m analysis[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mwhite()\u001b[38;5;241m.\u001b[39mscore(mate_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_EVAL)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNUM_ACTIONS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39mget_n_legal_moves()\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/chess/engine.py:2996\u001b[0m, in \u001b[0;36mSimpleEngine.analyse\u001b[0;34m(self, board, limit, multipv, game, info, root_moves, options)\u001b[0m\n\u001b[1;32m   2992\u001b[0m     coro \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mwait_for(\n\u001b[1;32m   2993\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39manalyse(board, limit, multipv\u001b[38;5;241m=\u001b[39mmultipv, game\u001b[38;5;241m=\u001b[39mgame, info\u001b[38;5;241m=\u001b[39minfo, root_moves\u001b[38;5;241m=\u001b[39mroot_moves, options\u001b[38;5;241m=\u001b[39moptions),\n\u001b[1;32m   2994\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_for(limit))\n\u001b[1;32m   2995\u001b[0m     future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(coro, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mloop)\n\u001b[0;32m-> 2996\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # play one episode\n",
    "    while not done:\n",
    "        # Convert the board \"obs\" to an 8 x 8 x 12 array i.e. all the bitboards..\n",
    "        #obs = board_to_obs(obs)\n",
    "        obs = obs.get_hash()\n",
    "        action = agent.get_action(obs)\n",
    "        take_action = env.legal_moves[action]\n",
    "        next_obs, reward, terminated, truncated, info = env.step(take_action)\n",
    "    \n",
    "        # update the agent\n",
    "        agent.update(obs, action, reward, terminated, next_obs.get_hash())\n",
    "        # update if the environment is done and the current obs\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "    agent.decay_epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nb = Board()\\nagent.epsilon = 0\\n\\nfor i in range(10):\\n    # 10 moves 5 by white, 5 by black\\n    obs = b.get_hash()\\n    g = Generator()\\n    g.generate_legal_moves(b)\\n    best = 0\\n\\n    if i % 2 == 0: # white to move\\n        best = agent.get_action(obs)\\n\\n    else: # black (random) agent\\n        best = np.random.randint(0, g.get_n_legal_moves())\\n\\n    b.make_move(g.get_legal_moves()[best])\\n    print_board(b)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "b = Board()\n",
    "agent.epsilon = 0\n",
    "\n",
    "for i in range(10):\n",
    "    # 10 moves 5 by white, 5 by black\n",
    "    obs = b.get_hash()\n",
    "    g = Generator()\n",
    "    g.generate_legal_moves(b)\n",
    "    best = 0\n",
    "\n",
    "    if i % 2 == 0: # white to move\n",
    "        best = agent.get_action(obs)\n",
    "\n",
    "    else: # black (random) agent\n",
    "        best = np.random.randint(0, g.get_n_legal_moves())\n",
    "\n",
    "    b.make_move(g.get_legal_moves()[best])\n",
    "    print_board(b)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
